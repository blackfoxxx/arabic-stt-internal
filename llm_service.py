"""
LLM Service Module for Arabic STT System
Integrates Ollama local LLM for text post-processing and enhancement
"""

import requests
import json
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import asyncio
import aiohttp

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class LLMConfig:
    """Configuration for LLM service - Optimized for RTX 5090"""
    base_url: str = "http://localhost:11434"
    model: str = "llama3.1:70b-instruct-q4_K_M"  # Upgraded to 70B for better Arabic understanding
    fallback_model: str = "llama3.1:8b"  # Keep 8B as fallback for high-volume processing
    dialect_model: str = "aya:35b-23-q4_K_M"  # Specialized for Arabic dialects
    timeout: int = 60  # Increased timeout for larger model
    max_retries: int = 3
    temperature: float = 0.1  # Lower temperature for more consistent results
    top_p: float = 0.9
    max_tokens: int = 2048

@dataclass
class LLMResponse:
    """Response from LLM service"""
    success: bool
    content: str
    error: Optional[str] = None
    processing_time: Optional[float] = None

class OllamaLLMService:
    """Service for interacting with Ollama LLM"""
    
    def __init__(self, config: LLMConfig = None):
        self.config = config or LLMConfig()
        self.session = None
        
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.config.timeout)
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    def is_available(self) -> bool:
        """Check if Ollama service is available"""
        try:
            response = requests.get(f"{self.config.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception as e:
            logger.error(f"Ollama service not available: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """Get list of available models"""
        try:
            response = requests.get(f"{self.config.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                return [model['name'] for model in data.get('models', [])]
            return []
        except Exception as e:
            logger.error(f"Error getting models: {e}")
            return []
    
    async def generate_text(self, prompt: str, system_prompt: str = None, model_override: str = None, use_dialect_model: bool = False) -> LLMResponse:
        """Generate text using Ollama LLM with model selection"""
        import time
        start_time = time.time()
        
        # Select appropriate model
        selected_model = self.config.model  # Default to primary (70B)
        if model_override:
            selected_model = model_override
        elif use_dialect_model and hasattr(self.config, 'dialect_model'):
            selected_model = self.config.dialect_model
        
        try:
            payload = {
                "model": selected_model,
                "prompt": prompt,
                "stream": False
            }
            
            # Add advanced parameters for better quality
            if hasattr(self.config, 'temperature'):
                payload["options"] = {
                    "temperature": self.config.temperature,
                    "top_p": self.config.top_p,
                    "num_predict": self.config.max_tokens
                }
            
            if system_prompt:
                payload["system"] = system_prompt
            
            if not self.session:
                self.session = aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=self.config.timeout)
                )
            
            async with self.session.post(
                f"{self.config.base_url}/api/generate",
                json=payload
            ) as response:
                
                if response.status == 200:
                    data = await response.json()
                    processing_time = time.time() - start_time
                    
                    return LLMResponse(
                        success=True,
                        content=data.get('response', ''),
                        processing_time=processing_time
                    )
                else:
                    error_text = await response.text()
                    # Try fallback model if primary fails
                    if selected_model == self.config.model and hasattr(self.config, 'fallback_model'):
                        logger.warning(f"Primary model failed, trying fallback: {self.config.fallback_model}")
                        return await self.generate_text(prompt, system_prompt, self.config.fallback_model)
                    
                    return LLMResponse(
                        success=False,
                        content='',
                        error=f"HTTP {response.status}: {error_text}"
                    )
                    
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Error generating text with {selected_model}: {e}")
            
            # Try fallback model if primary fails
            if selected_model == self.config.model and hasattr(self.config, 'fallback_model'):
                logger.warning(f"Primary model failed, trying fallback: {self.config.fallback_model}")
                return await self.generate_text(prompt, system_prompt, self.config.fallback_model)
            
            return LLMResponse(
                success=False,
                content='',
                error=str(e),
                processing_time=processing_time
            )
    
    def generate_text_sync(self, prompt: str, system_prompt: str = None, model_override: str = None, use_dialect_model: bool = False) -> LLMResponse:
        """Synchronous version of generate_text with model selection"""
        import time
        start_time = time.time()
        
        # Select appropriate model
        selected_model = self.config.model  # Default to primary (70B)
        if model_override:
            selected_model = model_override
        elif use_dialect_model and hasattr(self.config, 'dialect_model'):
            selected_model = self.config.dialect_model
        
        try:
            payload = {
                "model": selected_model,
                "prompt": prompt,
                "stream": False
            }
            
            # Add advanced parameters for better quality
            if hasattr(self.config, 'temperature'):
                payload["options"] = {
                    "temperature": self.config.temperature,
                    "top_p": self.config.top_p,
                    "num_predict": self.config.max_tokens
                }
            
            if system_prompt:
                payload["system"] = system_prompt
            
            response = requests.post(
                f"{self.config.base_url}/api/generate",
                json=payload,
                timeout=self.config.timeout
            )
            
            if response.status_code == 200:
                data = response.json()
                processing_time = time.time() - start_time
                
                return LLMResponse(
                    success=True,
                    content=data.get('response', ''),
                    processing_time=processing_time
                )
            else:
                # Try fallback model if primary fails
                if selected_model == self.config.model and hasattr(self.config, 'fallback_model'):
                    logger.warning(f"Primary model failed, trying fallback: {self.config.fallback_model}")
                    return self.generate_text_sync(prompt, system_prompt, self.config.fallback_model)
                
                return LLMResponse(
                    success=False,
                    content='',
                    error=f"HTTP {response.status_code}: {response.text}"
                )
                
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Error generating text with {selected_model}: {e}")
            
            # Try fallback model if primary fails
            if selected_model == self.config.model and hasattr(self.config, 'fallback_model'):
                logger.warning(f"Primary model failed, trying fallback: {self.config.fallback_model}")
                return self.generate_text_sync(prompt, system_prompt, self.config.fallback_model)
            
            return LLMResponse(
                success=False,
                content='',
                error=str(e),
                processing_time=processing_time
            )

class TextEnhancementService:
    """Service for text enhancement using LLM"""
    
    def __init__(self, llm_service: OllamaLLMService = None):
        self.llm_service = llm_service or OllamaLLMService()
    
    def correct_grammar(self, text: str, language: str = "arabic") -> LLMResponse:
        """Correct grammar and improve text quality with enhanced Iraqi Arabic support"""
        
        if language.lower() == "arabic":
            # Enhanced Arabic system prompt with Iraqi dialect awareness
            system_prompt = """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØµØ­ÙŠØ­ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¹Ø§Ù…ÙŠØ© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ:
1. ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù†Ø­ÙˆÙŠØ© ÙˆØ§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©
2. ØªØ­Ø³ÙŠÙ† ÙˆØ¶ÙˆØ­ Ø§Ù„Ù†Øµ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£ØµÙ„ÙŠ
3. Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø© Ù…Ø«Ù„: Ø´Ù„ÙˆÙ†ÙƒØŒ Ø´ÙƒÙˆ Ù…Ø§ÙƒÙˆØŒ Ø§ÙƒÙˆØŒ Ù…Ø§ÙƒÙˆØŒ ÙˆÙŠÙ†ØŒ Ø´Ù†ÙˆØŒ Ù‡Ø³Ù‡ØŒ Ø¬Ø§Ù†ØŒ ÙŠÙ…Ø¹ÙˆØ¯
4. ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© (Ú¯â†’Ù‚ØŒ Ú†â†’ÙƒØŒ Ú˜â†’Ø²ØŒ Ù¾â†’Ø¨)
5. ØªØ­Ø³ÙŠÙ† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ù„ØªÙ†Ø³ÙŠÙ‚
6. Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ø¶Ø±ÙˆØ±Ø© Ù„Ù„ÙˆØ¶ÙˆØ­

Ø§Ø­Ø±Øµ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙŠ Ø§Ù„ØªØµØ­ÙŠØ­."""
            
            prompt = f"""ØµØ­Ø­ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© ÙˆØ§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£ØµÙ„ÙŠ:

Ø§Ù„Ù†Øµ: {text}

Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØµØ­Ø­:"""
        else:
            # English system prompt
            system_prompt = """You are an expert text editor. Your task is to:
1. Correct grammar and spelling errors
2. Improve clarity while preserving original meaning
3. Fix punctuation and formatting
4. Maintain the original tone and style

Be precise and natural in your corrections."""
            
            prompt = f"""Please correct the following text while preserving its original meaning and tone:

Text: {text}

Corrected text:"""
        
        return self.llm_service.generate_text_sync(prompt, system_prompt)
    
    def summarize_text(self, text: str, language: str = "ar", max_length: int = 200) -> LLMResponse:
        """Summarize text with enhanced Arabic dialect support"""
        
        if language.startswith("ar"):
            # Enhanced Arabic system prompt
            system_prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¹Ø§Ù…ÙŠØ© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ:
1. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ ÙˆØ§Ø¶Ø­ ÙˆÙ…ÙÙŠØ¯ ÙÙŠ Ø­Ø¯ÙˆØ¯ {max_length} ÙƒÙ„Ù…Ø©
2. Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
3. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø©
4. Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£ØµÙ„ÙŠ
5. ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ ÙˆÙ…ØªØ³Ù„Ø³Ù„

Ø§Ø¬Ø¹Ù„ Ø§Ù„Ù…Ù„Ø®Øµ Ø´Ø§Ù…Ù„Ø§Ù‹ ÙˆÙ…ÙÙŠØ¯Ø§Ù‹."""
            
            prompt = f"""Ù„Ø®Øµ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙˆÙ…ÙÙŠØ¯:

Ø§Ù„Ù†Øµ: {text}

Ø§Ù„Ù…Ù„Ø®Øµ:"""
        else:
            # English system prompt
            system_prompt = f"""You are an expert text summarizer. Your task is to:
1. Create a clear and useful summary within {max_length} words
2. Preserve key points and important information
3. Use clear and understandable language
4. Maintain original context and meaning
5. Organize information logically

Make the summary comprehensive and useful."""
            
            prompt = f"""Please summarize the following text clearly and concisely:

Text: {text}

Summary:"""
        
        return self.llm_service.generate_text_sync(prompt, system_prompt)
    
    def translate_text(self, text: str, source_lang: str = "arabic", target_lang: str = "english") -> LLMResponse:
        """Translate text between languages"""
        if source_lang.lower() == "arabic" and target_lang.lower() == "english":
            system_prompt = "Ø£Ù†Øª Ù…ØªØ±Ø¬Ù… Ù…Ø­ØªØ±Ù Ù…Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©. Ù‚Ù… Ø¨ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ø¨Ø¯Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰."
            prompt = f"ØªØ±Ø¬Ù… Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:\n\n{text}"
        elif source_lang.lower() == "english" and target_lang.lower() == "arabic":
            system_prompt = "You are a professional translator from English to Arabic. Translate the text accurately while preserving the meaning."
            prompt = f"Translate the following text from English to Arabic:\n\n{text}"
        else:
            system_prompt = f"You are a professional translator. Translate the text from {source_lang} to {target_lang}."
            prompt = f"Translate the following text from {source_lang} to {target_lang}:\n\n{text}"
        
        return self.llm_service.generate_text_sync(prompt, system_prompt)
    
    def extract_keywords(self, text: str, language: str = "arabic", max_keywords: int = 10) -> LLMResponse:
        """Extract keywords from text"""
        if language.lower() == "arabic":
            system_prompt = f"Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ø§Ø³ØªØ®Ø±Ø¬ Ø£Ù‡Ù… {max_keywords} ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©."
            prompt = f"Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ:\n\n{text}"
        else:
            system_prompt = f"You are an AI assistant specialized in keyword extraction. Extract the top {max_keywords} keywords from the text."
            prompt = f"Extract keywords from the following text:\n\n{text}"
        
        return self.llm_service.generate_text_sync(prompt, system_prompt)

# Global service instances
llm_service = OllamaLLMService()
text_enhancement_service = TextEnhancementService(llm_service)

def test_llm_service():
    """Test function for LLM service"""
    print("Testing Ollama LLM Service...")
    
    # Check availability
    if not llm_service.is_available():
        print("âŒ Ollama service is not available")
        return False
    
    print("âœ… Ollama service is available")
    
    # Get available models
    models = llm_service.get_available_models()
    print(f"ğŸ“‹ Available models: {models}")
    
    # Test Arabic text generation
    test_text = "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ"
    print(f"\nğŸ§ª Testing with Arabic text: {test_text}")
    
    # Test grammar correction
    result = text_enhancement_service.correct_grammar(test_text)
    if result.success:
        print(f"âœ… Grammar correction: {result.content}")
    else:
        print(f"âŒ Grammar correction failed: {result.error}")
    
    # Test summarization
    long_text = "Ù‡Ø°Ø§ Ù†Ø¸Ø§Ù… Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«Ø©. ÙŠÙ…ÙƒÙ† Ù„Ù„Ù†Ø¸Ø§Ù… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ù…Ù†Ø·ÙˆÙ‚ Ø¥Ù„Ù‰ Ù†Øµ Ù…ÙƒØªÙˆØ¨ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©. ÙƒÙ…Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ø«Ù„ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØ­Ø¯Ø«ÙŠÙ† ÙˆØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØª."
    result = text_enhancement_service.summarize_text(long_text)
    if result.success:
        print(f"âœ… Summarization: {result.content}")
    else:
        print(f"âŒ Summarization failed: {result.error}")
    
    return True

if __name__ == "__main__":
    test_llm_service()