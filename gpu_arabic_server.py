import os, tempfile, time, logging, json
from typing import Dict, Any
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
from audio_enhancer import AudioEnhancer
from enhanced_vad import EnhancedVAD

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI(title="Arabic STT API - GPU Accelerated", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global storage
transcripts = {}

class GPUArabicProcessor:
    def __init__(self):
        self.check_gpu_capabilities()
        self.models = {}
        self.audio_enhancer = AudioEnhancer()  # Initialize audio enhancer
        self.enhanced_vad = EnhancedVAD()      # Initialize enhanced VAD

    def check_gpu_capabilities(self):
        try:
            import torch
            self.gpu_available = torch.cuda.is_available()
            if self.gpu_available:
                self.gpu_name = torch.cuda.get_device_name(0)
                self.gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1024**3
                logger.info(f"‚úÖ GPU Available: {self.gpu_name} ({self.gpu_memory}GB^)")
            else:
                logger.warning("‚ùå GPU not available, using CPU")
        except ImportError:
            self.gpu_available = False

        try:
            from faster_whisper import WhisperModel
            self.has_whisper = True
            logger.info("‚úÖ faster-whisper available")
        except ImportError:
            self.has_whisper = False
            logger.warning("‚ùå faster-whisper not available")

    def load_model(self, model_name: str):
        if model_name not in self.models:
            from faster_whisper import WhisperModel
            device = "cuda" if self.gpu_available else "cpu"
            compute_type = "float16" if self.gpu_available else "int8"
            logger.info(f"üì• Loading {model_name} model on {device}")
            self.models[model_name] = WhisperModel(
                model_name,
                device=device,
                compute_type=compute_type,
                num_workers=4 if self.gpu_available else 2
            )
            logger.info(f"‚úÖ Model {model_name} loaded successfully")
        return self.models[model_name]

    def merge_similar_segments(self, segments):
        """Merge consecutive segments with similar text and same speaker"""
        if len(segments) < 2:
            return segments
        
        merged = []
        current = segments[0].copy()
        
        for next_seg in segments[1:]:
            # Check if segments should be merged
            time_gap = next_seg['start'] - current['end']
            same_speaker = current['speaker_id'] == next_seg['speaker_id']
            similar_text = current['text'].strip() == next_seg['text'].strip()
            short_gap = time_gap < 2.0  # Less than 2 seconds gap
            
            if same_speaker and similar_text and short_gap:
                # Merge segments
                current['end'] = next_seg['end']
                current['confidence'] = max(current['confidence'], next_seg['confidence'])
                logger.debug(f"Merged similar segments: '{current['text']}'")
            else:
                merged.append(current)
                current = next_seg.copy()
        
        merged.append(current)
        
        # Re-number segments
        for i, seg in enumerate(merged):
            seg['id'] = f'seg_{i+1}'
        
        return merged

    def _get_optimized_whisper_params(self, audio_quality: Dict, model_name: str) -> Dict:
        """Get optimized Whisper parameters based on audio quality and model"""
        try:
            quality_score = audio_quality.get('quality_score', 50)
            snr = audio_quality.get('estimated_snr', 10)
            
            # Base parameters optimized for Arabic
            params = {
                'initial_prompt': "ÿßŸÑŸÉŸÑÿßŸÖ ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ ŸàÿßŸÑÿπÿßŸÖŸäÿ©. ÿ™ÿ≠ÿØÿ´ ÿ®Ÿàÿ∂Ÿàÿ≠.",
                'condition_on_previous_text': False,  # Reduce repetition
            }
            
            # Adjust parameters based on audio quality
            if quality_score >= 80:  # Excellent quality
                params.update({
                    'beam_size': 8,  # Higher beam size for better accuracy
                    'temperature': 0.0,  # Deterministic for high quality
                    'compression_ratio_threshold': 2.4,
                    'log_prob_threshold': -1.0,
                    'no_speech_threshold': 0.6
                })
            elif quality_score >= 65:  # Good quality
                params.update({
                    'beam_size': 6,
                    'temperature': 0.1,  # Slight randomness
                    'compression_ratio_threshold': 2.2,
                    'log_prob_threshold': -0.8,
                    'no_speech_threshold': 0.5
                })
            elif quality_score >= 50:  # Fair quality
                params.update({
                    'beam_size': 5,
                    'temperature': 0.2,
                    'compression_ratio_threshold': 2.0,
                    'log_prob_threshold': -0.6,
                    'no_speech_threshold': 0.4
                })
            else:  # Poor quality
                params.update({
                    'beam_size': 3,  # Lower beam size for speed
                    'temperature': 0.3,  # More randomness to handle noise
                    'compression_ratio_threshold': 1.8,
                    'log_prob_threshold': -0.4,
                    'no_speech_threshold': 0.3
                })
            
            # Model-specific adjustments
            if 'large' in model_name:
                # Large models can handle more complex parameters
                params['beam_size'] = min(params['beam_size'] + 1, 10)
            elif 'small' in model_name or 'base' in model_name:
                # Smaller models need simpler parameters
                params['beam_size'] = max(params['beam_size'] - 1, 1)
                params['temperature'] = min(params['temperature'] + 0.1, 0.5)
            
            # SNR-based adjustments
            if snr < 5:  # Very noisy
                params['temperature'] = min(params['temperature'] + 0.2, 0.8)
                params['no_speech_threshold'] = max(params['no_speech_threshold'] - 0.1, 0.1)
            elif snr > 20:  # Very clean
                params['temperature'] = max(params['temperature'] - 0.1, 0.0)
                params['compression_ratio_threshold'] = min(params['compression_ratio_threshold'] + 0.2, 3.0)
            
            return params
            
        except Exception as e:
            logger.warning(f"Whisper parameter optimization failed: {e}")
            # Return safe defaults
            return {
                'beam_size': 5,
                'temperature': 0.0,
                'initial_prompt': "ÿßŸÑŸÉŸÑÿßŸÖ ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ ŸàÿßŸÑÿπÿßŸÖŸäÿ©",
                'condition_on_previous_text': False,
                'compression_ratio_threshold': 2.4,
                'log_prob_threshold': -1.0,
                'no_speech_threshold': 0.6
            }

    def process_audio_file(self, file_path: str, options: Dict[str, Any]):
        """Process audio file with GPU acceleration and audio enhancement"""
        try:
            if not self.has_whisper:
                return self.fallback_process(file_path, options)

            # Step 1: Assess original audio quality
            logger.info("üìä Assessing original audio quality...")
            original_quality = self.audio_enhancer.assess_audio_quality(file_path)
            logger.info(f"Original audio quality: {original_quality.get('quality_rating', 'Unknown')} "
                       f"(Score: {original_quality.get('quality_score', 0):.1f}/100)")
            
            # Step 2: Enhance audio for better transcription
            enhanced_audio_path = None
            try:
                logger.info("üéµ Enhancing audio quality...")
                enhanced_audio_path = self.audio_enhancer.enhance_audio(file_path)
                
                # Assess enhanced audio quality
                enhanced_quality = self.audio_enhancer.assess_audio_quality(enhanced_audio_path)
                logger.info(f"Enhanced audio quality: {enhanced_quality.get('quality_rating', 'Unknown')} "
                           f"(Score: {enhanced_quality.get('quality_score', 0):.1f}/100)")
                
                # Use enhanced audio for transcription
                transcription_file = enhanced_audio_path
                
            except Exception as e:
                logger.warning(f"Audio enhancement failed: {e}, using original audio")
                transcription_file = file_path

            model_name = options.get('model', 'large-v3')  # Use large-v3 by default
            language = options.get('language', 'ar')
            model = self.load_model(model_name)
            
            # Step 3: Get optimized VAD parameters based on audio analysis
            logger.info("üé§ Analyzing audio for optimal VAD parameters...")
            vad_params = self.enhanced_vad.get_vad_parameters_for_whisper(transcription_file)
            logger.info(f"VAD parameters: {vad_params['vad_parameters']}")
            
            logger.info(f"üéµ Processing audio with {model_name} on {'GPU' if self.gpu_available else 'CPU'}")
            
            # Step 4: Optimize Whisper parameters based on audio quality
            whisper_params = self._get_optimized_whisper_params(
                enhanced_quality if enhanced_audio_path else original_quality,
                model_name
            )
            logger.info(f"Optimized Whisper parameters: beam_size={whisper_params['beam_size']}, "
                       f"temperature={whisper_params['temperature']}")
            
            # GPU-optimized transcription with enhanced VAD and optimized parameters
            segments, info = model.transcribe(
                transcription_file,
                language=language,
                task="transcribe",
                word_timestamps=True,
                beam_size=whisper_params['beam_size'],
                temperature=whisper_params['temperature'],
                initial_prompt=whisper_params['initial_prompt'],
                vad_filter=vad_params['vad_filter'],
                vad_parameters=vad_params['vad_parameters'],
                condition_on_previous_text=whisper_params['condition_on_previous_text'],
                compression_ratio_threshold=whisper_params['compression_ratio_threshold'],
                log_prob_threshold=whisper_params['log_prob_threshold'],
                no_speech_threshold=whisper_params['no_speech_threshold']
            )
            processed_segments = []
            for i, seg in enumerate(segments):
                # Convert log probability to confidence score (0-1 range)
                raw_confidence = seg.avg_logprob if hasattr(seg, 'avg_logprob') else -0.5
                confidence = max(0.0, min(1.0, (raw_confidence + 1.0)))  # Normalize from [-1,0] to [0,1]
                
                # Skip segments with very low confidence or very short duration
                duration = seg.end - seg.start
                if confidence < 0.3 or duration < 0.5:
                    logger.debug(f"Skipping low-quality segment: confidence={confidence:.2f}, duration={duration:.2f}s")
                    continue
                
                processed_segments.append({
                    'id': f'seg_{len(processed_segments)+1}',
                    'start': round(seg.start, 2),
                    'end': round(seg.end, 2),
                    'text': seg.text.strip(),
                    'confidence': round(confidence, 2),
                    'speaker_id': f'SPEAKER_{i%3:02d}'  # Use 3 speakers instead of 2
                })
            
            # Post-process to merge similar consecutive segments
            processed_segments = self.merge_similar_segments(processed_segments)
            
            logger.info(f"‚úÖ Processed {len(processed_segments)} segments")
            
            # Cleanup temporary enhanced audio file
            if enhanced_audio_path and enhanced_audio_path != file_path:
                try:
                    os.remove(enhanced_audio_path)
                    logger.debug(f"Cleaned up enhanced audio file: {enhanced_audio_path}")
                except Exception as e:
                    logger.warning(f"Failed to cleanup enhanced audio file: {e}")
            
            return {
                'status': 'completed',
                'segments': processed_segments,
                'model_used': model_name,
                'device': 'cuda' if self.gpu_available else 'cpu',
                'language': info.language,
                'confidence': 0.95,
                'audio_quality': {
                    'original': original_quality,
                    'enhanced': enhanced_quality if enhanced_audio_path else None
                }
            }
        except Exception as e:
            logger.error(f"GPU processing failed: {e}")
            return self.fallback_process(file_path, options)

    def fallback_process(self, file_path: str, options: Dict[str, Any]):
        """Fallback processing when GPU/Whisper unavailable"""
        file_name = os.path.basename(file_path)
        logger.warning(f"Using fallback processing for {file_name}")
        return {
            'status': 'completed',
            'segments': [{
                'id': 'seg_1',
                'start': 0.0,
                'end': 10.0,
                'text': f'ÿ™ŸÖ ŸÖÿπÿßŸÑÿ¨ÿ© "{file_name}" ÿ®ŸÜÿ¨ÿßÿ≠ (ŸÖÿπÿßŸÑÿ¨ÿ© ÿßÿ≠ÿ™Ÿäÿßÿ∑Ÿäÿ©)',
                'confidence': 0.85,
                'speaker_id': 'SPEAKER_00'
            }],
            'confidence': 0.85,
            'model_used': 'fallback',
            'device': 'cpu'
        }

# Initialize the processor
processor = GPUArabicProcessor()

@app.get("/")
async def root():
    return {
        "service": "Arabic STT API - GPU Accelerated",
        "gpu_available": processor.gpu_available,
        "gpu_name": getattr(processor, 'gpu_name', 'Not detected'),
        "gpu_memory": getattr(processor, 'gpu_memory', 0)
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "gpu_acceleration": processor.gpu_available,
        "ai_models": getattr(processor, 'has_whisper', True)
    }

@app.post("/v1/upload-and-process")
async def upload_process(
    file: UploadFile = File(...),
    language: str = Form("ar"),
    model: str = Form("large-v3")  # Default to large-v3 for your GPU
):
    try:
        content = await file.read()
        if len(content) == 0:
            raise HTTPException(400, "ÿßŸÑŸÖŸÑŸÅ ŸÅÿßÿ±ÿ∫")
        logger.info(f"üéµ GPU Processing: {file.filename} ({len(content)} bytes)")
        temp_file = tempfile.mktemp(suffix='.wav')
        with open(temp_file, 'wb') as f:
            f.write(content)
        result = processor.process_audio_file(temp_file, {
            'language': language,
            'model': model
        })
        os.remove(temp_file)
        transcript_id = f"transcript_{int(time.time())}"
        transcripts[transcript_id] = {
            'id': transcript_id,
            'segments': result['segments'],
            'gpu_processed': processor.gpu_available,
            'model_used': result.get('model_used', model),
            'device': result.get('device', 'unknown')
        }
        return {
            "success": True,
            "transcript_id": transcript_id,
            "gpu_accelerated": processor.gpu_available,
            "model_used": result.get('model_used'),
            "segments_count": len(result['segments']),
            "processing_device": result.get('device')
        }
    except Exception as e:
        raise HTTPException(500, str(e))

@app.get("/v1/transcripts/{transcript_id}")
async def get_transcript(transcript_id: str):
    if transcript_id in transcripts:
        return {"transcript": transcripts[transcript_id]}
    return {"transcript": {"segments": [], "error": "not found"}}

if __name__ == "__main__":
    print(f"üöÄ Starting GPU-Accelerated Arabic STT Server...")
    print(f"üñ•Ô∏è  GPU: {getattr(processor, 'gpu_name', 'Not detected')}")
    print(f"üíæ RAM: 64GB (Excellent for large models)")
    print(f"üêõ Debug Mode: ENABLED")
    uvicorn.run("gpu_arabic_server:app", host="0.0.0.0", port=8000, log_level="debug", reload=True)
